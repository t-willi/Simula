{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyGshyT4DuJyvkxINkl9aT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t-willi/Simula/blob/main/PTB_patho.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install ecg_plot\n",
        "!pip install wfdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d8ZbVFDgr8L6",
        "outputId": "2abf5d18-39b4-4ef3-d9f3-db8d81f34992"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 6.4 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 68.5 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 58.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 75.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 76.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 70.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 73.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 76.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 73.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 73.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 75.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8808 sha256=641f40c8d9558847c8ef258549c0fc1b57b23af949364caa4d4acd532dbf9b97\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ecg_plot\n",
            "  Downloading ecg_plot-0.2.8-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: ecg-plot\n",
            "Successfully installed ecg-plot-0.2.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.0.0-py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.7.3)\n",
            "Requirement already satisfied: SoundFile<0.12.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.11.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.3.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0.0,>=3.2.2->wfdb) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.2.2->wfdb) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.10)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "from pathlib import Path\n",
        "# current_position=pathlib.PurePath(__file__)\n",
        "# dir=current_position.parent\n",
        "# main_folder=dir.joinpath(\"main_folder\")\n",
        "current_position=pathlib.PurePath(\"/content/\")\n",
        "main_folder=current_position.joinpath(\"main_folder\")\n",
        "Path(main_folder).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for artifacts\n",
        "artifact_dir=main_folder.joinpath(\"artifacts\")\n",
        "Path(artifact_dir).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for train_data\n",
        "train_dir=main_folder.joinpath(\"train_dir\")\n",
        "Path(train_dir).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for ecg_files\n",
        "ecg_dir=main_folder.joinpath(\"ecg\")\n",
        "Path(ecg_dir).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for saved model sate dicts\n",
        "model_dir=main_folder.joinpath(\"model\")\n",
        "Path(model_dir).mkdir(parents=False,exist_ok = True)\n",
        "\n",
        "\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import torch.optim as optim\n",
        "from random import shuffle\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "if torch.cuda.is_available()==True:\n",
        "  device=\"cuda:0\"\n",
        "else:\n",
        "  device =\"cpu\"\n",
        "wandb.login(key=\"7a8ee9d41cc2d51eb77fd795e14f74a215e63c2d\")\n",
        "api = wandb.Api()\n",
        "artifact = api.artifact('ecg_simula/upload_PTB_XL_patho.zip/PTB_patho:v0', type='dataset')\n",
        "artifact.download(artifact_dir)\n",
        "model = api.artifact('ecg_simula/pTOP_PTB_normal/Model:v53', type='Model')\n",
        "model.download(artifact_dir)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "def request(path=None,name=None):\n",
        "  import requests\n",
        "  from pathlib import Path\n",
        "  request = requests.get(path)\n",
        "  name=name+\".py\"\n",
        "  with open(name,\"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "unzip_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/unzip.py\"\n",
        "Dataloader_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/dataloader_PTB_normal_1to7.py\"\n",
        "get_pred_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/get_pred_12Lead.py\"\n",
        "plt_ECG_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/plot_ECG_12Lead.py\"\n",
        "\n",
        "request(unzip_git_dir,\"Unzip\")\n",
        "from Unzip import unzip\n",
        "\n",
        "request(Dataloader_git_dir,\"dataset_and_loader\")\n",
        "from dataset_and_loader import Custom_dataset_PTB as CD\n",
        "from dataset_and_loader import make_loader as ml\n",
        "\n",
        "\n",
        "\n",
        "#download prediction generator\n",
        "request(get_pred_git_dir,\"get_predictions\")\n",
        "from get_predictions import get_pred_12lead as get_pred\n",
        "#download ECG plotter\n",
        "request(plt_ECG_git_dir,\"plot_ECG\")\n",
        "from plot_ECG import plotECG_12Lead as plotECG\n",
        "\n",
        "artifact_dir_str=str(artifact_dir.joinpath(\"PTB_patho.zip\"))\n",
        "unzip(save_path=train_dir,zip_path=artifact_dir_str,reload=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tys3eGLEsS6Y",
        "outputId": "643b1d7d-53dc-45d3-da02-90abd25dc441"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact PTB_patho:v0, 1075.66MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/main_folder/train_dir directory exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "class Transpose1dLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=11, upsample=None, output_padding=1):\n",
        "        super(Transpose1dLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        self.upsample_layer = torch.nn.Upsample(scale_factor=upsample)\n",
        "        reflection_pad = kernel_size // 2\n",
        "        self.reflection_pad = nn.ConstantPad1d(reflection_pad, value=0)\n",
        "        self.conv1d = torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride)\n",
        "        self.Conv1dTrans = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            #x = torch.cat((x, in_feature), 1)\n",
        "            return self.conv1d(self.reflection_pad(self.upsample_layer(x)))\n",
        "        else:\n",
        "            return self.Conv1dTrans(x)\n",
        "\n",
        "\n",
        "class Pulse2pulseGenerator(nn.Module):\n",
        "    def __init__(self,latent_dim=100, post_proc_filt_len=512,upsample=True):\n",
        "        super(Pulse2pulseGenerator, self).__init__()\n",
        "        # \"Dense\" is the same meaning as fully connection.\n",
        "        stride = 4\n",
        "        if upsample:\n",
        "            stride = 1\n",
        "            upsample = 5\n",
        "        # if upsample is anything but none Transpose1dLayer will do\n",
        "        # self.conv1d(self.reflection_pad(self.upsample_layer(x)))\n",
        "        # which is a 1d convolution on padded and upsampled data x\n",
        "        self.deconv_1 = Transpose1dLayer(250 , 250, 25, stride, upsample=upsample)\n",
        "        self.deconv_2 = Transpose1dLayer(250, 150, 25, stride, upsample=upsample)\n",
        "        self.deconv_3 = Transpose1dLayer(150, 50, 25, stride, upsample=upsample)\n",
        "        self.deconv_4 = Transpose1dLayer( 50, 25, 25, stride, upsample=2)\n",
        "        self.deconv_5 = Transpose1dLayer( 25, 10, 25, stride, upsample=upsample)\n",
        "        self.deconv_6 = Transpose1dLayer(  10, 7, 25, stride, upsample=2)\n",
        "\n",
        "\n",
        "        #new convolutional layers\n",
        "        self.conv_1 = nn.Conv1d(1, 10, 25, stride=2, padding=25 // 2)\n",
        "        self.conv_2 = nn.Conv1d(10, 25, 25, stride=5, padding= 25 // 2)\n",
        "        self.conv_3 = nn.Conv1d(25, 50 , 25, stride=2, padding= 25 // 2)\n",
        "        self.conv_4 = nn.Conv1d(50, 150 , 25, stride=5, padding= 25 // 2)\n",
        "        self.conv_5 = nn.Conv1d(150, 250 , 25, stride=5, padding= 25 // 2)\n",
        "        self.conv_6 = nn.Conv1d(250, 250 , 25, stride=5, padding= 25 // 2)\n",
        "        self.flatt = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(500,100)\n",
        "        self.linear2 = nn.Linear(100,500)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.ConvTranspose1d) or isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, LS=False):\n",
        "        self.LS=LS\n",
        "        if x.ndim==2:\n",
        "          x=x.unsqueeze(0)\n",
        "        x = F.leaky_relu(self.conv_1(x)) #(1,1,5000 --> 1, 10, 2500)\n",
        "        x = F.leaky_relu(self.conv_2(x)) #( --> 1, 25, 500)\n",
        "        x = F.leaky_relu(self.conv_3(x)) #(--> 1, 50, 250)\n",
        "        x = F.leaky_relu(self.conv_4(x)) # --> 1, 150, 50)\n",
        "        x = F.leaky_relu(self.conv_5(x)) #(--> 1, 250, 10)\n",
        "        x = F.leaky_relu(self.conv_6(x)) #(--> 1, 250, 2)-->flatten into (1,500)), then to linear ((1,100)), and then back\n",
        "        x = self.flatt(x) # (1,500)\n",
        "        LS = self.linear1(x) #(1,100)\n",
        "        if self.LS is True:\n",
        "          return LS\n",
        "        x = self.linear2(LS) #(1,500)\n",
        "        zero_dim=x.shape[0]\n",
        "        x=torch.reshape(x,(zero_dim,250,2)) #1(1,250,2)\n",
        "        x = F.relu(self.deconv_1(x)) #(--> 1, 250, 10)\n",
        "        x = F.relu(self.deconv_2(x)) #(--> 1, 150, 50)\n",
        "        x = F.relu(self.deconv_3(x)) #( --> 1, 50, 250)\n",
        "        x = F.relu(self.deconv_4(x)) #(--> 1, 25, 500)\n",
        "        x = F.relu(self.deconv_5(x)) #(--> 1, 10, 2500)\n",
        "        x = torch.tanh(self.deconv_6(x)) #(1, 7, 5000)\n",
        "        x=x.squeeze()\n",
        "        return x\n",
        "\n",
        "model=Pulse2pulseGenerator().to(device)\n",
        "artifact_dir_string=str(artifact_dir.joinpath(\"model\"))\n",
        "model_path= artifact_dir_string\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOhxCSZtebq",
        "outputId": "1b50d2c1-92e8-4a99-929b-58490bacba93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcaevov6nSjt"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# 🐝 initialise a wandb run\n",
        "config = dict(\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.0001,)  #learing rate from puls to puls paper\n",
        "\n",
        "\n",
        "def model_pipeline(hyperparameters,model=model):\n",
        "    # tell wandb to get started\n",
        "    wandb.init(project=\"pTOP_PTB_patho_test\", config=hyperparameters)\n",
        "    # access all HPs through wandb.config, so logging matches execution!\n",
        "    config = wandb.config\n",
        "    # make the model, data, and optimization problem\n",
        "    train_loader, val_loader,test_dataset, criterion, optimizer,val_dataset = make(config)\n",
        "    # and use them to train the model\n",
        "    train(model, train_loader,val_loader,test_dataset, criterion, optimizer,val_dataset,config)\n",
        "    return model\n",
        "\n",
        "def make(config):\n",
        "    # Make the data\n",
        "    print(\"making data\")\n",
        "    data_dir=str(train_dir)\n",
        "    train_dataset = CD(data_dir=data_dir,split=True,target=\"train\",size=1)\n",
        "    val_dataset = CD(data_dir=data_dir,split=True,target=\"val\",size=1)\n",
        "    test_dataset = CD(data_dir=data_dir,split=True,target=\"test\",size=1)\n",
        "    train_loader = ml(train_dataset, batch_size=config.batch_size)\n",
        "    val_loader = ml(val_dataset, batch_size=config.batch_size)\n",
        "    \n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    \n",
        "    return train_loader, val_loader,test_dataset, criterion, optimizer,val_dataset\n",
        "\n",
        "\n",
        "def train(model, train_loader,val_loader,test_dataset, criterion, optimizer,val_dataset, config):\n",
        "  # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "  wandb.watch(model, criterion, log=\"all\")\n",
        "  for epoch in tqdm((range(config.epochs))):\n",
        "    train_loss=0\n",
        "    for batch,(X,y) in tqdm((enumerate(train_loader))):\n",
        "      # Forward pass ➡\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      #print(f\"shape of input{x.shape},shape of label_y{y.shape}\") \n",
        "      model.train()\n",
        "      #print(f\"x.shape is{X.max()} y.shape is {y.max()}\")\n",
        "      output=model(X)\n",
        "      #print(f\"output is {output.max()}\")\n",
        "      #print(f\"shape of model_output_raw{output.shape}\") \n",
        "      # output=torch.reshape(output,(config.batch_size, 1, 7, 5000))\n",
        "      loss = criterion(output,y)\n",
        "      train_loss += loss\n",
        "      # Backward pass ⬅\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      # Step with optimizer\n",
        "      optimizer.step()\n",
        "    #average loss per batch\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,(X,y) in tqdm(enumerate(val_loader)):\n",
        "        #print(\"doing test loop\")\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        val_pred = model(X)\n",
        "        # val_pred=torch.reshape(val_pred,(config.batch_size, 1, 7, 5000))\n",
        "        loss=criterion(val_pred,y)\n",
        "        val_loss += loss\n",
        "      val_loss /= len(val_loader)  \n",
        "      wandb.log({\"train_loss\": train_loss, \n",
        "                 \"val_loss\": val_loss,\n",
        "                 \"Epoch\":epoch})\n",
        "      \n",
        "\n",
        "    if (epoch) % 1==0:\n",
        "      df_input,df_output=get_pred(dataset=val_dataset,model=model,random=True,upscale=33)\n",
        "      model.to(device)\n",
        "      #plotting the ECG and creating the combined DF\n",
        "      combined_df=plotECG(df_input,df_output,path=str(ecg_dir))\n",
        "      #saving combined DF as table on wandB\n",
        "      input_prediction_table = wandb.Table(dataframe=combined_df)\n",
        "      ecg_dir_file=ecg_dir.joinpath(\"ecg.png\")\n",
        "      wandb.log({\"ECG\": wandb.Image(str(ecg_dir_file))})\n",
        "      wandb.log({\"Input and predictions\": input_prediction_table}) \n",
        "      \n",
        "    if (epoch) % 1==0:\n",
        "      print(\"one\")\n",
        "      model_dir_model=model_dir.joinpath(\"model\")\n",
        "      torch.save(model.state_dict(),str(model_dir_model))\n",
        "      print(\"two\")\n",
        "      wandb.log_artifact(str(model_dir_model), name='Model', type='Model')\n",
        "\n",
        "model = model_pipeline(config)\n",
        "\n"
      ]
    }
  ]
}