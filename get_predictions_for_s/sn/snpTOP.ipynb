{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAYBkc90OBqwBr624YZSVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t-willi/Simula/blob/main/get_predictions_for_s/sn/snpTOP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SZhL9CEfBDHw",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5f9b0e-f464-4d73-c716-3076f9129de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 33.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 61.1 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 62.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 54.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 57.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 57.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 59.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 53.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 67.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 59.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 50.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=2ee215a6ab0bf0e1a280de78e9a08fb01ab66fdbb3c7f30d2f4db22814903ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ecg_plot\n",
            "  Downloading ecg_plot-0.2.8-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: ecg-plot\n",
            "Successfully installed ecg-plot-0.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install ecg_plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rd4f-cYuBfkl"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from pathlib import Path\n",
        "# current_position=pathlib.PurePath(__file__)\n",
        "# dir=current_position.parent\n",
        "# main_folder=dir.joinpath(\"main_folder\")\n",
        "current_position=pathlib.PurePath(\"/content/\")\n",
        "main_folder=current_position.joinpath(\"main_folder\")\n",
        "Path(main_folder).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for artifacts\n",
        "artifact_dir=main_folder.joinpath(\"artifacts\")\n",
        "Path(artifact_dir).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for train_data\n",
        "train_dir=main_folder.joinpath(\"train_dir\")\n",
        "Path(train_dir).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for ecg_files\n",
        "ecg_dir=main_folder.joinpath(\"ecg\")\n",
        "Path(ecg_dir).mkdir(parents=False,exist_ok = True)\n",
        "#create folder and directory for saved model sate dicts\n",
        "model_dir=main_folder.joinpath(\"model\")\n",
        "Path(model_dir).mkdir(parents=False,exist_ok = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "19G_1YgVBm62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5faa53-952b-4a67-90bc-73aaad9501b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f529bed1450>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import torch.optim as optim\n",
        "from random import shuffle\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "if torch.cuda.is_available()==True:\n",
        "  device=\"cuda:0\"\n",
        "else:\n",
        "  device =\"cpu\"\n",
        "\n",
        "apikey = \"7a8ee9d41cc2d51eb77fd795e14f74a215e63c2d\"\n",
        "api = wandb.Api()\n",
        "test_artifact = api.artifact('ecg_simula/upload_Model_test_data/Test_data:v0', type='dataset')\n",
        "test_artifact.download(\"/content/artifacts/test_data\")\n",
        "pTOP_1_7_syn = api.artifact('ecg_simula/AE_pTOP_serverrun_synthetic_data/Model:v6', type='Model')\n",
        "## pTOP_1_7_norm comes from training of pTOP on PTB_normal data with pretraining from ecg_simula/AE_pTOP_serverrun_synthetic_data/Model:v3\n",
        "pTOP_1_7_norm = api.artifact('ecg_simula/pTOP_PTB_normal/Model:v54', type='Model')\n",
        "## model trained with pathological data\n",
        "pTOP_1_7_patho = api.artifact('ecg_simula/pTOP_PTB_patho/Model:v38', type='Model')\n",
        "## download model states \n",
        "#models for ecg prediciton\n",
        "pTOP_1_7_syn.download(\"/content/artifacts/pTOP_1_7_syn\")\n",
        "pTOP_1_7_norm.download(\"/content/artifacts/pTOP_1_7_norm\")\n",
        "pTOP_1_7_patho.download(\"/content/artifacts/pTOP_1_7_patho\")\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6_tEBvjPCWuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ebb5d8-31b1-401d-f723-78706915d776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find /content/test_data directory, creating one...\n",
            "Unzipping data to folder...\n",
            "unzip is finished\n"
          ]
        }
      ],
      "source": [
        "# previous_model = api.artifact('ecg_simula/AE_pTOP_LS_optim/Model:v8', type='Model')\n",
        "# previous_model.download()\n",
        "\n",
        "def request(path=None,name=None):\n",
        "  import requests\n",
        "  from pathlib import Path\n",
        "  request = requests.get(path)\n",
        "  name=name+\".py\"\n",
        "  with open(name,\"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "unzip_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/unzip.py\"\n",
        "get_pred=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/get_pred_12Lead.py\"\n",
        "plt_ECG_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/plot_ECG_12Lead.py\"\n",
        "Dataloader_syn_1to7_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/dataloader_syn_1to7.py\"\n",
        "Dataloader_normal_1to7_git_dir=\"https://raw.githubusercontent.com/t-willi/NeuralNetworks/main/dataloader_PTB_normal_1to7.py\"\n",
        "\n",
        "request(unzip_git_dir,\"Unzip\")\n",
        "from Unzip import unzip\n",
        "\n",
        "#request(pTOP_git_dir,\"pTOP\")\n",
        "#from pTOP import *\n",
        "request(Dataloader_syn_1to7_git_dir,\"dataset_and_loader\")\n",
        "from dataset_and_loader import make_loader as ml\n",
        "request(Dataloader_syn_1to7_git_dir,\"syn1_7\")\n",
        "from syn1_7 import Custom_dataset as CD_syn_1_7\n",
        "request(Dataloader_normal_1to7_git_dir,\"normal1_7\")\n",
        "from normal1_7 import Custom_dataset_PTB as CD_normal_1_7\n",
        "#download prediction generator\n",
        "request(get_pred,\"get_predictions\")\n",
        "from get_predictions import get_pred_12lead as get_pred\n",
        "#download ECG plotter\n",
        "request(plt_ECG_git_dir,\"plot_ECG\")\n",
        "from plot_ECG import plotECG_12Lead as plotECG\n",
        "\n",
        "unzip(save_path=\"/content/test_data\",zip_path=\"/content/artifacts/test_data/ecg_prediction_data.zip\",reload=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "class Transpose1dLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=11, upsample=None, output_padding=1):\n",
        "        super(Transpose1dLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        self.upsample_layer = torch.nn.Upsample(scale_factor=upsample)\n",
        "        reflection_pad = kernel_size // 2\n",
        "        self.reflection_pad = nn.ConstantPad1d(reflection_pad, value=0)\n",
        "        self.conv1d = torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride)\n",
        "        self.Conv1dTrans = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            #x = torch.cat((x, in_feature), 1)\n",
        "            return self.conv1d(self.reflection_pad(self.upsample_layer(x)))\n",
        "        else:\n",
        "            return self.Conv1dTrans(x)\n",
        "\n",
        "\n",
        "class pTOP_1to7(nn.Module):\n",
        "    def __init__(self,latent_dim=100, post_proc_filt_len=512,upsample=True):\n",
        "        super(pTOP_1to7, self).__init__()\n",
        "        # \"Dense\" is the same meaning as fully connection.\n",
        "        stride = 4\n",
        "        if upsample:\n",
        "            stride = 1\n",
        "            upsample = 5\n",
        "        # if upsample is anything but none Transpose1dLayer will do\n",
        "        # self.conv1d(self.reflection_pad(self.upsample_layer(x)))\n",
        "        # which is a 1d convolution on padded and upsampled data x\n",
        "        self.deconv_1 = Transpose1dLayer(250 , 250, 25, stride, upsample=upsample)\n",
        "        self.deconv_2 = Transpose1dLayer(250, 150, 25, stride, upsample=upsample)\n",
        "        self.deconv_3 = Transpose1dLayer(150, 50, 25, stride, upsample=upsample)\n",
        "        self.deconv_4 = Transpose1dLayer( 50, 25, 25, stride, upsample=2)\n",
        "        self.deconv_5 = Transpose1dLayer( 25, 10, 25, stride, upsample=upsample)\n",
        "        self.deconv_6 = Transpose1dLayer(  10, 7, 25, stride, upsample=2)\n",
        "\n",
        "\n",
        "        #new convolutional layers\n",
        "        self.conv_1 = nn.Conv1d(1, 10, 25, stride=2, padding=25 // 2)\n",
        "        self.conv_2 = nn.Conv1d(10, 25, 25, stride=5, padding= 25 // 2)\n",
        "        self.conv_3 = nn.Conv1d(25, 50 , 25, stride=2, padding= 25 // 2)\n",
        "        self.conv_4 = nn.Conv1d(50, 150 , 25, stride=5, padding= 25 // 2)\n",
        "        self.conv_5 = nn.Conv1d(150, 250 , 25, stride=5, padding= 25 // 2)\n",
        "        self.conv_6 = nn.Conv1d(250, 250 , 25, stride=5, padding= 25 // 2)\n",
        "        self.flatt = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(500,100)\n",
        "        self.linear2 = nn.Linear(100,500)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.ConvTranspose1d) or isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, LS=False):\n",
        "        self.LS=LS\n",
        "        if x.ndim==2:\n",
        "          x=x.unsqueeze(0)\n",
        "        x = F.leaky_relu(self.conv_1(x)) #(1,1,5000 --> 1, 10, 2500)\n",
        "        x = F.leaky_relu(self.conv_2(x)) #( --> 1, 25, 500)\n",
        "        x = F.leaky_relu(self.conv_3(x)) #(--> 1, 50, 250)\n",
        "        x = F.leaky_relu(self.conv_4(x)) # --> 1, 150, 50)\n",
        "        x = F.leaky_relu(self.conv_5(x)) #(--> 1, 250, 10)\n",
        "        x = F.leaky_relu(self.conv_6(x)) #(--> 1, 250, 2)-->flatten into (1,500)), then to linear ((1,100)), and then back\n",
        "        x = self.flatt(x) # (1,500)\n",
        "        LS = self.linear1(x) #(1,100)\n",
        "        if self.LS is True:\n",
        "          return LS\n",
        "        x = self.linear2(LS) #(1,500)\n",
        "        zero_dim=x.shape[0]\n",
        "        x=torch.reshape(x,(zero_dim,250,2)) #1(1,250,2)\n",
        "        x = F.relu(self.deconv_1(x)) #(--> 1, 250, 10)\n",
        "        x = F.relu(self.deconv_2(x)) #(--> 1, 150, 50)\n",
        "        x = F.relu(self.deconv_3(x)) #( --> 1, 50, 250)\n",
        "        x = F.relu(self.deconv_4(x)) #(--> 1, 25, 500)\n",
        "        x = F.relu(self.deconv_5(x)) #(--> 1, 10, 2500)\n",
        "        x = torch.tanh(self.deconv_6(x)) #(1, 7, 5000)\n",
        "        x=x.squeeze()\n",
        "        return x\n",
        "\n",
        "model_syn=pTOP_1to7().to(device)\n",
        "model_norm=pTOP_1to7().to(device)\n",
        "model_patho=pTOP_1to7().to(device)\n",
        "# PATH=\"/content/artifacts/Model:v3/model\"\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "NMLiWd3pWgh9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data to run trough the pTOP7to7 for feature extraction. A sample from syn data will be run trough trained pTOP1to7 models, the 7 lead output will be run trough a pTOP7to7 model and its latent space compared to the latent space of the real 7 leads.\n"
      ],
      "metadata": {
        "id": "wyOtGwE0Uf5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "class Custom_dataset_syn():\n",
        "    def __init__(self, data_dir,max_value=5011,column=3,split=True,target=\"train\",size=1):\n",
        "      #get all files from directory loaded in all_files list\n",
        "      self.column=column\n",
        "      self.max_value=max_value\n",
        "      self.size=size\n",
        "      #should shuffle the data here?\n",
        "      self.files = glob.glob(data_dir + '/*.csv')\n",
        "      self.len=int((len(self.files))*self.size)\n",
        "      #print(f\"len:{self.len}\")\n",
        "      self.cut1=int(self.len*0.8)\n",
        "      #print(f\"cut1:{self.cut1}\")\n",
        "      self.cut2=int(self.len*0.9)\n",
        "      #print(f\"cut2:{self.cut2}\")\n",
        "      self.train_files=self.files[0:self.cut1]\n",
        "      self.test_files=self.files[self.cut1:self.cut2]\n",
        "      self.val_files=self.files[self.cut2:self.len]\n",
        "      self.target=target\n",
        "      self.split=split\n",
        "\n",
        "    def __len__(self):\n",
        "      if self.split is True:\n",
        "        if self.target is \"train\":\n",
        "          return len(self.train_files)\n",
        "        if self.target is \"test\":\n",
        "          return len(self.test_files)\n",
        "        if self.target is \"val\":\n",
        "          return len(self.val_files)\n",
        "      if self.split is not True:\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "      header=[\"I\", \"II\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\"]\n",
        "      #turn list of dataframes into Tensor\n",
        "      if self.split is True:\n",
        "        if self.target is \"train\":\n",
        "          temp_df=pd.read_csv(self.train_files[idx],index_col=0,header=0,names=header)\n",
        "        if self.target is \"test\":\n",
        "          temp_df=pd.read_csv(self.test_files[idx],index_col=0,header=0,names=header)\n",
        "        if self.target is \"val\":\n",
        "          temp_df=pd.read_csv(self.val_files[idx],index_col=0,header=0,names=header)\n",
        "      if self.split is not True:\n",
        "        temp_df=pd.read_csv(self.files[idx],index_col=0,header=0,names=header)\n",
        "      temp_df/=self.max_value\n",
        "      #load input tensor\n",
        "      \n",
        "      temp_list_in=temp_df.loc[:,\"I\"]\n",
        "      #temp_list_in=normalize([temp_list_in], norm=\"max\")\n",
        "      temp_tensor_in = torch.tensor(temp_list_in,dtype=torch.float32)\n",
        "      temp_tensor_in=temp_tensor_in.unsqueeze(0)\n",
        "      #load label Tensor\n",
        "      temp_list_out=temp_df.loc[:,[\"II\",\"v1\",\"v2\",\"v3\",\"v4\",\"v5\",\"v6\"]].values\n",
        "      #temp_list_out=normalize([temp_list_out], norm=\"max\")\n",
        "      temp_tensor_out=torch.tensor(temp_list_out,dtype=torch.float32)\n",
        "      temp_tensor_out=temp_tensor_out.T\n",
        "      #combine input and label and output\n",
        "      temp_tensor_pair= temp_tensor_in,temp_tensor_out\n",
        "      return temp_tensor_pair\n",
        "\n",
        "def make_loader(dataset,batch_size):\n",
        "  from torch.utils.data import DataLoader\n",
        "  loader = DataLoader(dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      drop_last=True\n",
        "                      )\n",
        "  return loader"
      ],
      "metadata": {
        "id": "StmzqMggIa3t"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "class Custom_dataset_PTB():\n",
        "    def __init__(self, data_dir,max_value=33,column=3,split=True,target=\"train\",size=1):\n",
        "      #get all files from directory loaded in all_files list\n",
        "      self.column=column\n",
        "      self.max_value=max_value\n",
        "      self.size=size\n",
        "      #should shuffle the data here?\n",
        "      self.files = glob.glob(data_dir + '/*.csv')\n",
        "      self.len=int((len(self.files))*self.size)\n",
        "      #print(f\"len:{self.len}\")\n",
        "      self.cut1=int(self.len*0.8)\n",
        "      #print(f\"cut1:{self.cut1}\")\n",
        "      self.cut2=int(self.len*0.9)\n",
        "      #print(f\"cut2:{self.cut2}\")\n",
        "      self.train_files=self.files[0:self.cut1]\n",
        "      self.test_files=self.files[self.cut1:self.cut2]\n",
        "      self.val_files=self.files[self.cut2:self.len]\n",
        "      self.target=target\n",
        "      self.split=split\n",
        "\n",
        "    def __len__(self):\n",
        "      if self.split is True:\n",
        "        if self.target is \"train\":\n",
        "          return len(self.train_files)\n",
        "        if self.target is \"test\":\n",
        "          return len(self.test_files)\n",
        "        if self.target is \"val\":\n",
        "          return len(self.val_files)\n",
        "      if self.split is not True:\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "      header=[\"I\", \"II\", \"III\", \"aVF\", \"aVR\", \"aVL\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\"]\n",
        "      #turn list of dataframes into Tensor\n",
        "      if self.split is True:\n",
        "        if self.target is \"train\":\n",
        "          temp_df=pd.read_csv(self.train_files[idx],index_col=0,header=0,names=header)\n",
        "        if self.target is \"test\":\n",
        "          temp_df=pd.read_csv(self.test_files[idx],index_col=0,header=0,names=header)\n",
        "        if self.target is \"val\":\n",
        "          temp_df=pd.read_csv(self.val_files[idx],index_col=0,header=0,names=header)\n",
        "      if self.split is not True:\n",
        "        temp_df=pd.read_csv(self.files[idx],index_col=0,header=0,names=header)\n",
        "      temp_df/=self.max_value\n",
        "      #load input tensor\n",
        "      \n",
        "      temp_list_in=temp_df.loc[:,\"I\"]\n",
        "      #temp_list_in=normalize([temp_list_in], norm=\"max\")\n",
        "      temp_tensor_in = torch.tensor(temp_list_in,dtype=torch.float32)\n",
        "      temp_tensor_in=temp_tensor_in.unsqueeze(0)\n",
        "      #load label Tensor\n",
        "      temp_list_out=temp_df.loc[:,[\"II\",\"v1\",\"v2\",\"v3\",\"v4\",\"v5\",\"v6\"]].values\n",
        "      #temp_list_out=normalize([temp_list_out], norm=\"max\")\n",
        "      temp_tensor_out=torch.tensor(temp_list_out,dtype=torch.float32)\n",
        "      temp_tensor_out=temp_tensor_out.T\n",
        "      #combine input and label and output\n",
        "      temp_tensor_pair= temp_tensor_in,temp_tensor_out\n",
        "      return temp_tensor_pair\n",
        "\n",
        "def make_loader(dataset,batch_size):\n",
        "  from torch.utils.data import DataLoader\n",
        "  loader = DataLoader(dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      drop_last=True\n",
        "                      )\n",
        "  return loader"
      ],
      "metadata": {
        "id": "KY4F-ADiJFb1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn_test_data=Custom_dataset_syn(\"/content/test_data/Syn\",split=False)\n",
        "STTC_test_data=Custom_dataset_PTB(\"/content/test_data/STTC\",split=False)\n",
        "Norm_test_data=Custom_dataset_PTB(\"/content/test_data/Norm\",split=False)\n",
        "MI_test_data=Custom_dataset_PTB(\"/content/test_data/MI\",split=False)\n",
        "HYP_test_data=Custom_dataset_PTB(\"/content/test_data/HYP\",split=False)\n",
        "CD_test_data=Custom_dataset_PTB(\"/content/test_data/CD\",split=False)\n",
        "datasets={\"syn_test_data\":syn_test_data,\n",
        "          \"STTC_test_data\":STTC_test_data,\n",
        "          \"Norm_test_data\":Norm_test_data,\n",
        "          \"MI_test_data\":MI_test_data,\n",
        "          \"HYP_test_data\":HYP_test_data,\n",
        "          \"CD_test_data\":CD_test_data\n",
        "          }\n"
      ],
      "metadata": {
        "id": "3vYj6W5kIueq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1pOfbpDfnLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#set up 1to7 models for prediction\n",
        "syn1_7_path=\"/content/artifacts/pTOP_1_7_syn/model\"\n",
        "model_syn.load_state_dict(torch.load(syn1_7_path, map_location=device))\n",
        "norm1_7_path=\"/content/artifacts/pTOP_1_7_norm/model\"\n",
        "model_norm.load_state_dict(torch.load(norm1_7_path, map_location=device))\n",
        "patho1_7_path=\"/content/artifacts/pTOP_1_7_patho/model\"\n",
        "model_patho.load_state_dict(torch.load(patho1_7_path, map_location=device))\n",
        "models={\"model_syn\":model_syn,\n",
        "        \"model_norm\":model_norm,\n",
        "        \"model_patho\":model_patho\n",
        "        }"
      ],
      "metadata": {
        "id": "AfFFQxlGWEix"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get prediction from dataset for synthetic data, this is for testing purposes to see that the ecg looks right.\n",
        "dataset=\"HYP_test_data\"\n",
        "for model in models:\n",
        "  for k,files in enumerate(datasets[dataset]):\n",
        "    temp_in,temp_out=get_pred_12lead(random=False,Set=files,model=models[model],upscale=33)\n",
        "    combined_df=plotECG(temp_in,temp_out,title=f\"{k}_{model}\",path=f\"/content/ecg_predictions/{dataset}\",scale=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "QUT4Kd-hV-ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"ecg_predictions\", 'zip', \"/content/ecg_predictions\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i6IMf2zXVu6Q",
        "outputId": "014a2301-c9cc-4ac1-e621-7c50e4c31828"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ecg_predictions.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0lGHAC7Qpk-"
      },
      "outputs": [],
      "source": []
    }
  ]
}